# -*- coding: utf-8 -*-
"""3_Predictive Analytics Classification - Titanic Survival Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mtYFj7DattB7vYN5tu-bBaWbrqHyrR2s

# Data exploration and feature engineering for predictive analytics

Survival classification using [Titanic dataset](https://www.kaggle.com/c/titanic/data).

Dataset:  
1. You need to upload the titanic train and test datasets to Google Colab.  
2. You should click on the '>' icon to the left (below the title bar), expand the left menu.  
3. Select Files tab.  
4. Click upload to upload the tp3_titanic_data.csv file.  
Note: You need to upload these datasets everytime you reconnect to the notebook.  

Step 1:  Data Exploration.  

Step 2:  Data cleaning and Feature Engineering  

Step 3:  Predictive Modeling using Decision Tree

Step 4:  Predictive Modeling using Neural Network

Step 5:  Accuracy Evaluation

![alt text](https://static1.squarespace.com/static/5006453fe4b09ef2252ba068/t/5090b249e4b047ba54dfd258/1351660113175/TItanic-Survival-Infographic.jpg?format=600w)

## Load data
"""

# Commented out IPython magic to ensure Python compatibility.
# Data manipulation libraries
import pandas as pd
import numpy as np

# Visualization Libraries
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
sns.set() # setting seaborn default for plots

# Make sure you upload the two data files as instructed above.
df = pd.read_csv('tp3_titanic_data.csv')

"""## Step 1: Data Exploration"""

df.head()

"""For your reference, the Data Dictionary is as follows :

Survived: 0 = No, 1 = Yes  (what we are trying to predict)

pclass: Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd  
sibsp: # of siblings/spouse aboard  
parch: # of parents/children aboard  
ticket: Ticket number  
cabin: Cabin number  
embarked: Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton  
"""

print('Train Shape (rows, columns):', df.shape)

df.info()

# Check missing values
df.isnull().sum()

"""We can see that Age attribute is missing in many records/vectors. Only 714/891 contain a value for Age.

Similarly, Cabin values - 204/ 891  
"""

# Let's write a function to create a column chart with respect to outcome feature.
# i.e. survival based on a given feature type. (i.e. survival based on gender)
def bar_chart(dataframe, feature):
    survived = dataframe[dataframe['Survived']==1][feature].value_counts()
    dead = dataframe[dataframe['Survived']==0][feature].value_counts()
    df = pd.DataFrame([survived,dead])
    df.index = ['Survived','Dead']
    df.plot(kind='bar',stacked=True, figsize=(10,5))

bar_chart(df, 'Sex')

"""This chart conveys more women than men survived."""

bar_chart(df, 'Pclass')

"""
This chart conveys first class mostly survived, and third class mostly died.
"""

bar_chart(df, 'SibSp')

"""What does this chart tell you?"""

bar_chart(df, 'Parch')

"""What does this chart tell you?"""

bar_chart(df, 'Embarked')

"""What does this chart tell you?

Also pay attention to how the function works:



```
def bar_chart(dataframe, feature):

    survived = dataframe[dataframe['Survived']==1][feature].value_counts()
    dead = dataframe[dataframe['Survived']==0][feature].value_counts()
    df = pd.DataFrame([survived,dead])
    df.index = ['Survived','Dead']
    df.plot(kind='bar',stacked=True, figsize=(10,5))
```

## Step 2: Wrangling and Feature Engineering

### Extracting salutation (title) from passenger name

Passenger name is not useful for this classification task.  
However, deriving the salutation (or title) of each passenger from the name could be useful  (e.g., Mrs, Mr, Ms, Dr.)
"""

# Extract titles
# The regex will extract the string before period sign.
df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\.', expand=False)

df['Title'].value_counts()

# Standardize (normalise) titles in a meaningful manner
# i.e., we reduce all 17 titles to only 4 (Mr, Miss, Mrs and Others).
title_mapping = {"Mr": 0, "Miss": 1, "Mrs": 2,
                 "Master": 3, "Dr": 3, "Rev": 3, "Col": 3, "Major": 3, "Mlle": 3,"Countess": 3,
                 "Ms": 3, "Lady": 3, "Jonkheer": 3, "Don": 3, "Dona" : 3, "Mme": 3,"Capt": 3,"Sir": 3 }
df['Title'] = df['Title'].map(title_mapping)

df['Title'].value_counts()

bar_chart(df, 'Title')

# delete unnecessary feature (i.e., Name) from dataset
df.drop('Name', axis=1, inplace=True)

"""Why did we remove name and kept title separately?  (Hint: Nominal variables)

### Transforming gender from categorical to numerical attribute.

Transform Sex column to numeric format.
"""

# What do we have?
df['Sex'].unique()

sex_mapping = {"male": 0, "female": 1}
df['Sex'] = df['Sex'].map(sex_mapping)

bar_chart(df, 'Sex')

"""### Missing value imputation and Binning

Missing value imputation for the Age attribute. Use median age based on the title attribute for missing Age.
"""

df.groupby("Title")["Age"].transform("median")

# fill missing age with median age for each title (Mr, Mrs, Miss, Others)
df["Age"].fillna(df.groupby("Title")["Age"].transform("median"), inplace=True)

# Age distribution
facet = sns.FacetGrid(df, hue="Survived",aspect=4)
facet.map(sns.kdeplot,'Age',shade= True)
facet.set(xlim=(0, df['Age'].max()))
facet.add_legend()

"""Binning/Converting Numerical Age to Categorical Variable.

feature vector map:  
child: 0  
young: 1  
adult: 2  
mid-age: 3  
senior: 4  
"""

df.loc[ df['Age'] <= 16, 'Age'] = 0
df.loc[(df['Age'] > 16) & (df['Age'] <= 26), 'Age'] = 1
df.loc[(df['Age'] > 26) & (df['Age'] <= 36), 'Age'] = 2
df.loc[(df['Age'] > 36) & (df['Age'] <= 62), 'Age'] = 3
df.loc[ df['Age'] > 62, 'Age'] = 4

bar_chart(df, 'Age')

"""Missing value imputation for Embarked"""

Pclass1 = df[df['Pclass']==1]['Embarked'].value_counts()
Pclass2 = df[df['Pclass']==2]['Embarked'].value_counts()
Pclass3 = df[df['Pclass']==3]['Embarked'].value_counts()
df_combined = pd.DataFrame([Pclass1, Pclass2, Pclass3])
df_combined.index = ['1st class','2nd class', '3rd class']
df_combined.plot(kind='bar',stacked=True, figsize=(10,5))

"""more than 50% of 1st class are from S embark  
more than 50% of 2nd class are from S embark  
more than 50% of 3rd class are from S embark  

***fill out missing embark with S embark***
"""

df['Embarked'] = df['Embarked'].fillna('S')

df.head()

embarked_mapping = {"S": 0, "C": 1, "Q": 2}
df['Embarked'] = df['Embarked'].map(embarked_mapping)

"""Missing value imputation and Binning for Fare"""

# fill missing Fare with median fare for each Pclass
df["Fare"].fillna(df.groupby("Pclass")["Fare"].transform("median"), inplace=True)

df.head(5)

# Binning
df.loc[ df['Fare'] <= 17, 'Fare'] = 0
df.loc[(df['Fare'] > 17) & (df['Fare'] <= 30), 'Fare'] = 1
df.loc[(df['Fare'] > 30) & (df['Fare'] <= 100), 'Fare'] = 2
df.loc[ df['Fare'] > 100, 'Fare'] = 3

df.head(5)

"""Standardizing and missing value imputation for Cabin"""

df.Cabin.value_counts()

df['Cabin'] = df['Cabin'].str[:1]

Pclass1 = df[df['Pclass']==1]['Cabin'].value_counts()
Pclass2 = df[df['Pclass']==2]['Cabin'].value_counts()
Pclass3 = df[df['Pclass']==3]['Cabin'].value_counts()
df_combined = pd.DataFrame([Pclass1, Pclass2, Pclass3])
df_combined.index = ['1st class','2nd class', '3rd class']
df_combined.plot(kind='bar', stacked=True, figsize=(10,5))

cabin_mapping = {"A": 0, "B": 0.4, "C": 0.8, "D": 1.2, "E": 1.6, "F": 2, "G": 2.4, "T": 2.8}
df['Cabin'] = df['Cabin'].map(cabin_mapping)

# fill missing Fare with median fare for each Pclass
df["Cabin"].fillna(df.groupby("Pclass")["Cabin"].transform("median"), inplace=True)  # Use median value for replacement

"""### Delete unused features

Note: Ticket, SibSp and Parch are potential good features. For the workshop, we will remove them however, you may use your knowledge on predictive analytics to identify a way to use them, in order to improve the accuracy.
"""

df.head(5)

"""**Note: You may run the following line of code only once (to drop columns).**  
Because once a column is dropped, it will no longer be part of the dataset. Then reattempting to drop the came columns will cause an error as python cannot find such column names in the dataset.
"""

features_drop = ['Ticket', 'SibSp', 'Parch', 'PassengerId']
df = df.drop(features_drop, axis=1)

"""### Data Split"""

from sklearn.model_selection  import train_test_split

Y_train = df['Survived']
X_train = df.drop('Survived', axis=1)

X_train, X_validation, Y_train, Y_validation = train_test_split(X_train, Y_train, test_size = 0.33, random_state = 5)

print('Train Shape (rows, columns):', X_train.shape)
print('Validation Shape (rows, columns):', X_validation.shape)

"""## Step 3: Modelling - Decision Tree Classification"""

from sklearn.tree import DecisionTreeClassifier

df.info()

"""There are number of parameters that can be fine-tuned for improved accuracy.  
Parameters can be found in the API: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html
"""

clf = DecisionTreeClassifier()
clf.fit(X_train, Y_train)
Y_predict = clf.predict(X_validation)

"""### Accuracy Evaluation

We consider 3 metrics to evaluate this classification model.  
1. Accuracy  (API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html)  
2. Confusion Matrix  (API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)  
3. Precision and Recall  (API: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html)
"""

# Evaluate Accuracy
from sklearn.metrics import accuracy_score
accuracy = accuracy_score(Y_validation, Y_predict)
print('Validation Accuracy: {:.2f}%'.format(accuracy*100))

from sklearn.metrics import confusion_matrix
confusion_matrix(Y_validation, Y_predict, labels=[0, 1])

# Evaluate precision and recall
from sklearn.metrics import precision_recall_fscore_support
precision, recall, f_score, support = precision_recall_fscore_support(Y_validation, Y_predict, labels=[0, 1])
print('precision: {}(0), {}(1)'.format(precision[0], precision[1]))
print('recall: {}(0), {}(1)'.format(recall[0], recall[1]))
print('f_score: {}(0), {}(1)'.format(f_score[0], f_score[1]))
print('support: {}(0), {}(1)'.format(support[0], support[1]))

"""### Visualize the decision tree  
We would need following libraries to display the tree.  
Note that the column names are ordered as they appear in the input dataframe.  
i.e., (X1=Pclass, X2=Sex, X3=Age, X4=Fare, X5=Cabin, X6=Embarked, X7=Title)
"""

from six import StringIO
from IPython.display import Image
from sklearn.tree import export_graphviz
import pydotplus

dot_data = StringIO()
export_graphviz(clf, out_file=dot_data,
                filled=True, rounded=True,
                special_characters=True)

graph = pydotplus.graph_from_dot_data(dot_data.getvalue())
Image(graph.create_png())

X_train.info()

"""##Step 4: Modelling - Neural Network Classification"""

from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(hidden_layer_sizes=(20), max_iter=1000)

mlp.fit(X_train, Y_train)

Y_predict = mlp.predict(X_validation)

"""### Accuracy Evaluation"""

# Evaluate Accuracy
accuracy = accuracy_score(Y_validation, Y_predict)
print('Validation Accuracy: {:.2f}%'.format(accuracy*100))

confusion_matrix(Y_validation, Y_predict, labels=[0, 1])

# Evaluate precision and recall
precision, recall, f_score, support = precision_recall_fscore_support(Y_validation, Y_predict, labels=[0, 1])
print('precision: {}(0), {}(1)'.format(precision[0], precision[1]))
print('recall: {}(0), {}(1)'.format(recall[0], recall[1]))
print('f_score: {}(0), {}(1)'.format(f_score[0], f_score[1]))
print('support: {}(0), {}(1)'.format(support[0], support[1]))

"""##Accuracy Analysis
  
  Decision Tree vs. Neural Network Classification  
    
  ![alt text](https://i.ibb.co/WPmgm2R/classification-accuracy.png)

## Reference Documents

*   [Titanic: Machine Learning from Disaster](https://www.kaggle.com/c/titanic)
*   [Titanic Solution with sklearn classifiers](https://www.kaggle.com/minsukheo/titanic-solution-with-sklearn-classifiers)
"""